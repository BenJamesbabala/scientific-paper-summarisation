<html>
<head>

	<title>Paper</title>
	<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">

	<style type="text/css">

	html {
		font-family: 'Source Code Pro', monospace;
	}

	.text {
		margin-bottom: 50px;
	}

	.container {
		text-align: center;
		margin-left: auto;
		margin-right: auto;
		max-width: 70%;
	}
	</style>

</head>

<body>
<div class="container">
<div id="title" class="text"><h1>statistical estimation of the names of https servers with domain name graphs</h1></div><div id="gold" class="text"><h2>Human Written Summary</h2><hr><br><p>we present the domain name graph ( dng ) , which is a formal expression that can keep track of cname chains and characterize the dynamic and diverse nature of dns mechanisms and deployments .<br><br>we develop a framework called service-flow map ( sfmap ) that works on top of the dng .<br><br>sfmap estimates the hostname of an https server when given a pair of client and server ip addresses .<br><br>it can statistically estimate the hostname even when associating dns queries are unobserved due to caching mechanisms , etc through extensive analysis using real packet traces , we demonstrate that the sfmap framework establishes good estimation accuracies and can out- perform the state-of-the art technique called dn-hunter .<br><br>we also identify the optimized setting of the sfmap framework .<br><br>the experiment results suggest that the success of the sfmap lies in the fact that it can complement incomplete dns information by leveraging the graph structure .<br><br>to cope with large-scale measurement data , we introduce techniques to make the sfmap framework scalable .<br><br>we validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links .<br><br></p></div><div id="paper" class="text"><h2>Full Paper</h2><hr><br><br><br><h3>MAIN-TITLE</h3><p>statistical estimation of the names of https servers with domain name graphs<br><br></p><br><br><h3>KEYPHRASES</h3><p>traffic analysis ssl/tls dns graph<br><br></p><br><br><h3>INTRODUCTION</h3><p>background : monitoring and understanding traffic mix is crucial for network operators .<br><br>port number conventions and deep packet inspection ( dpi ) are widely used to understand the breakdown of traffic mix .<br><br>however , these techniques have become less effective for the following reasons .<br><br>first , the majority of modern services , such as social networking service , video , and messaging services , are all performed over web traffic [ 15 ] , and port number information is too coarse-grained to distinguish such services from each other .<br><br>second , the encryption of communication channels has disabled inspection of http headers , which include useful information such as uniform resource identifiers ( uris ) .<br><br>modern protocols for accelerating the web such as spdy and websocket employ mandatory encryption of http with ssl/tls ( secure socket layer/transport layer security ) , ie , https .<br><br>naylor etal [ 13 ] recently reported that fraction of https traffic volume measured at a large-scale isp has significantly increased over these 2+ years ( from april 2012 to july 2014 ) .<br><br>they also found that their measurement study suggests that cost of deploying https is decreasing .<br><br>in fact , the internet survey data collected by the [ 17 ] shows that as of september 2015 , more than 68 % of the alexa top 1m web sites [ 1 ] deploy ssl/tls to encrypt their traffic .<br><br>hence , the increasing adoption of https brings new research challenges to traffic classification problems as discussed in the past studies such as [ 2,7 ] 11we note that server name indication ( sni ) extension of tls can be used to obtain hostname of https server .<br><br>however , there are many client/server implementations that do not adopt sni .<br><br>in fact , in our dataset , roughly half of https clients did not use the sni extension.. goal and challenges : based on the aforementioned information , this work aims to enable network operators to infer the hostnames of https traffic .<br><br>hostname information is useful for network operators to understand what types of services are carried over https flows .<br><br>although the ip address property of an https server may reveal that the server is used by a particular company such as google , this information often fails to provide us with information about the services that are used over the flow , such as web searches , blogs , and videos .<br><br>such services are associated with distinct hostnames such as www.google.com , www.blogspot.com , and www.youtube.com .<br><br>bermudez etal [ 2 ] revealed that simple reverse dns lookup does not return accurate domain information used by https servers .<br><br>thus , to understand the traffic mix of https flows , we need to infer server hostnames .<br><br>the main idea of our approach is to correlate https flows and dns queries/responses .<br><br>the basic assumption is that prior to requesting an https flow , a web application should resolve the ip address of the https server by querying a dns query .<br><br>therefore , by monitoring prior dns queries/responses , we can estimate the hostname that is associated with ip address of the https server .<br><br>although this approach might look trivial , there are three practical challenges .<br><br>( challenge 1 ) canonical name ( cname ) tricks used by cdns first , modern cdn providers leverage cname tricks to accelerate the efficiency of content delivery [ 16 ] .<br><br>fig1 shows an example of a cname chain used by a cdn provider .<br><br>here , assume that we know that the ip address of an observed https server is s1=23.2.132.181 .<br><br>now , our task is to associate s1 with the original hostname , n1= www.ieee.org .<br><br>however , as is shown in fig1 , n1 is not directly resolved to s1 due to the existence of the cname chain .<br><br>using this chain structure , a cdn provider can provide the optimal server ip address s1 to serve the content of n1 to client c1 .<br><br>thus , to associate s1 and n1 , we need to keep track of the cname chain , which exhibits dynamic and complex behavior as we shall see soon .<br><br>( challenge 2 ) incomplete measurements a dns record can be cached by several mechanisms such as local dns resolvers , dns caching within operating systems , and dns caching within applications such as web browsers .<br><br>the implementations of these caching mechanisms are diverse .<br><br>some recent implementations used in web browsers store dns records aggressively to improve response time , thereby ignoring dns ttl settings [ 4 ] .<br><br>even though such implementations violate the rule of dns ttl , they can work because even if a selected server ip address is no longer an optimal one , the server ip address generally continues to be valid .<br><br>thus , due to the standard and illicit caching mechanisms , a dns query , which should have appeared prior to an http request , is often invisible .<br><br>the absence of dns queries suggests that we require estimation techniques to recover incomplete measurements .<br><br>( challenge 3 ) dynamicity , diversity , and ambiguity every hostname used in dns is assigned a time-to-live ( ttl ) , which defines the lifetime of the hostname within a stub dns resolver .<br><br>if the hostname is not queried again before the ttl has expired , the dns record of the hostname will be removed from a stub dns resolver .<br><br>in general , the hostnames in a cname chain have different ttl values .<br><br>fig2 presents an example of cumulative distributive function ( cdf ) of ttl values for hostnames that are resolved to ip addresses ( a record ) and hostnames that are resolved to cnames ( cname record ) .<br><br>note that the data was taken from a mid-sized production network , and the characteristics of cdf were the same for other dataset .<br><br>the graph clearly shows that a record hostnames have shorter ttls than cname hostnames .<br><br>for example , more than 50 % of a record hostnames have ttl values that are less than 60s .<br><br>this indicates that the association between hostnames and ip addresses is highly dynamic .<br><br>these hostnames have shorter ttls because cdn providers tend to control traffic at a fine granularity [ 4 ] .<br><br>the diversity of ttl values and dns caching mechanisms leads to ambiguity of cname association behavior .<br><br>we illustrate an actual sample in fig3 , which presents dns resolutions for a client , c1 .<br><br>the first observation generates the relationship between s1 and n1 for client c1 .<br><br>the second observation generates the relationship between s2 and n2 for client c1 .<br><br>now , assume an estimation problem .<br><br>if we observe the pair ( c1 , s1 ) , which hostname should it be associated with ?<br><br>if we simply keep the relationships shown above , the answer is n1 .<br><br>however , due to the existence of intermediate cname node m1 , the actual answer is n2 because m1 is now associated with s2 by a query of n2 , and n1 is associated with m1 due to a caching mechanism .<br><br>note that this behavior depends on the implementation of the stub dns resolver used by the client c1 .<br><br>if the implementation ignores intermediate cname nodes , the answer could be n1 .<br><br>thus , there is an intrinsic ambiguity in cname associations .<br><br>contributions : in this work , we present a novel methodology that aims to infer the hostnames of https flows , given the three research challenges shown above .<br><br>the key contributions of this work are summarized as follows .<br><br><span style="background-color:hsla(20.556745182, 100%, 50%, 0.5)">&nbspwe present the domain name graph ( dng ) , which is a formal expression that can keep track of cname chains ( challenge 1 ) and characterize the dynamic and diverse nature of dns mechanisms and deployments ( challenge 3 ) . </span><br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspwe develop a framework called service-flow map ( sfmap ) that works on top of the dng . </span><br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspsfmap estimates the hostname of an https server when given a pair of client and server ip addresses . </span><br><br>it can statistically estimate the hostname even when associating dns queries are unobserved due to caching mechanisms , etc ( challenge 2 ) .<br><br>through extensive analysis using real packet traces , we demonstrate that the sfmap framework establishes good estimation accuracies and can outperform the state-of-the art technique called dn-hunter , [ 2 ] .<br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspwe also identify the optimized setting of the sfmap framework . </span><br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspthe experiment results suggest that the success of the sfmap lies in the fact that it can complement incomplete dns information by leveraging the graph structure . </span><br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspto cope with large-scale measurement data , we introduce techniques to make the sfmap framework scalable . </span><br><br><span style="background-color:hsla(0.0, 100%, 50%, 0.5)">&nbspwe validate the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links . </span><br><br>the remainder of this paper is organized as follows : section2 summarizes the related work .<br><br>section3 describes the proposed sfmap framework in detail .<br><br>in section4 , using the small and mid-size data , we perform performance evaluation to identify the optimized setting of the sfmap framework .<br><br>section5 proposes techniques that make the sfmap framework scalable .<br><br>we also validate the effectiveness of the approach using large-scale traffic data .<br><br>section6 discusses the limitations of sfmap and future research directions .<br><br>finally , we conclude our work in section7 .<br><br></p><br><br><h3>RELATED WORK</h3><p>many studies have examined the internet traffic classification problem .<br><br>68 studies on the topic is listed in [ 3 ] .<br><br>here , we focus our attention on the studies that make use of dns information to the traffic classification problem ; [ 2,10,14 ] .<br><br>mori etal [ 10 ] proposed a method to identify traffic originating from large-scale video-sharing services such as youtube .<br><br>the key idea was to extract the rules of ip address numbering and naming conventions of fully qualified domain names ( fqdns ) used for the services .<br><br>although their approach may work for a limited scope , it can not be used to solve more generic web traffic classification problems .<br><br>plonka and barford [ 14 ] presented a traffic classification method that uses dns traffic .<br><br>they developed a method that stores per client dns rendezvous state information in a tree-like data structure .<br><br>although their results demonstrated that the dns rendezvous-based method performs well , even for encrypted traffic , their goal was different from ours because they assumed that dns traffic implies the ground truth .<br><br>in contrast , our goal is to estimate the hostnames of https traffic from the observations of dns traffic .<br><br>bermudez etal [ 2 ] developed a framework called dn-hunter , which aims to classify traffic flows using dns traffic .<br><br>dn-hunter uses a fifo ( first-in first-out ) circular list to store the relationships among fqdn information and client-server pairs .<br><br>since the scope of dn-hunter is mostly similar to ours , this work compares the performance of sfmap with dn-hunter .<br><br>this section describes sfmap in detail .<br><br>section3.1 presents the overview of the sfmap framework .<br><br>section3.2 describes dng , which is a key component of the sfmap framework .<br><br>section3.3 details how sfmap estimates hostnames .<br><br>lastly , section3.4 explains how sfmap updates dng and statistics that are used for the estimation .<br><br></p><br><br><h3>OVERVIEW</h3><p>the goal of sfmap is to infer a hostname n of an https flow by associating preceding dns responses with a flow key , which is defined with a pair of server ip address s and client ip address c. to this end , sfmap needs to address the research challenges discussed in section1 .<br><br>to tackle the research challenges , the sfmap framework works on top of dng , which will be detailed in the next subsection .<br><br>a dng keeps track of the structure of dns records ; thus , it can deal with cname chains ( challenge 1 ) .<br><br>next , by relaxing the constraints of the dng , the sfmap framework can handle cases wherein there are no preceding dns responses that are associated with the client-server pair ( challenge 2 ) .<br><br>the details of the hostname estimation will be described in section3.3 .<br><br>finally , by adequately maintaining the dng and using the observed ttl values , the sfmap framework can deal with the dynamic nature of dns mechanisms ( challenge 3 ) .<br><br>the updating mechanism for the dng will be discussed in section3.4 .<br><br>fig4 summarizes the components of the sfmap framework .<br><br>sfmap has three main functions , ie , learner , estimator , and updater .<br><br>learner consists of two components : the dng and the frequency counter .<br><br>learner component reads dns queries/responses and builds and keeps the dng and frequency counter .<br><br>estimator performs host estimation ; ie , given a pair of client-server ip addresses ( c , s ) for an https flow , estimator returns the most plausible hostname ( s ) using the information collected from dng and frequency counters .<br><br>updater reads dns queries/responses and updates the status of the dng and the frequency counter .<br><br>given these primitives , our problem can be formulated as maximum likelihood estimation ( mle ) under the constraints of a dng .<br><br>given c and s in an https flow , the mle is formulated as follows .<br><br>( 1 ) n^ ( c , s ) =argmaxnnpr ( n , c , s ) ( 2 ) s.t.n= { nvc : ngcs } , where gc= ( vc , ec ) denotes a dng built for c , and binary operator xgy represents whether vertex x can reach to vertex y on graph g. in the following , we describe how we build and update gc , how we extract n , how we compute the likelihood probability pr ( n , c , s ) , and how we get the final estimation n^ .<br><br>a dng , gc , is a directed graph used to keep a and cname records observed in dns responses queried by client c. dngs can be built separately for each client c. a vertex , v vc , is a server ip address or a hostname , while an edge , e ec , represents an a or cname record that links a vertex to another vertex .<br><br>each edge is grafted by a corresponding a or cname record observed in a dns response , and is associated with its expire time determined by observed ttl .<br><br>if an edge , e ec , is expired , it will be removed from gc .<br><br>here , we examine how the dng expression naturally represents the behavior of dns resolution .<br><br>assume that clients obtain a server address via dns responses only and that we have never missed any dns response for the clients ; ie , dng gc represents all name resolutions requested by a client c. when a client c sends an http request to a server n , the server ns ip address s should have been resolved by dns .<br><br>this association of n and s obtained through the dns mechanism can be expressed as a path from n to s on the dng gc .<br><br>note that there are cases where we can not find such a path due to the caching mechanisms .<br><br>in such cases , we need to employ several techniques that will be described soon .<br><br>in the estimation phase , we must first select candidate hostnames that are likely the original hostname for a given client-server pair ( c , s ) .<br><br>we extract a set of candidate hostnames n from dng gc , using eq2 .<br><br>if |n| 1 , we estimate the hostname with the mle shown in eq1 .<br><br>a method to calculate the likelihood probability pr ( n , c , s ) will be shown later .<br><br>as we mentioned in section1 , n can be an empty set due to the standard and illicit dns caching mechanisms .<br><br>in such cases , we can not directly associate an https flow with preceding dns responses .<br><br>to deal with these cases , sfmap extends the candidate hostnames by relaxing the constraint of edge expiration .<br><br>this relaxation enables us to select hostnames that are missed due to the existence of dns clients that ignore dns ttl for improving the user experience .<br><br>now , n is obtained as ( 3 ) n= { nvc : ngcs } , where gc= ( vc , ec ) and ec include both valid and expired edges .<br><br>finally , if we do not have any candidate hostnames at this stage , we use the union of all clients dngs .<br><br>we call such dng as union dng .<br><br>we also call the original dns as local dng .<br><br>while local dng considers per-client status such as ttl expiration , union dng ignores them .<br><br>however , union dng can cover the cases when a dns query is not observed for a client .<br><br>in other words , we use the observations of other clients as a hint to estimate the most plausible hostname .<br><br>let c denote a set of all clients .<br><br>the union dng is defined as g= ( v=ccvc , e=ccec ) .<br><br>using the union dng g , the candidate hostnames can be selected as ( 4 ) n= { nv : ngs } .it then estimates the hostname with the following mle formulation : ( 5 ) n^=argmaxnnpr ( n , s ) .like eq3 , we can further relax the constraint of expiration for the union dng g ; ie , ( 6 ) n= { nv : ngs } , where g= ( v , e ) and e include both valid and expired edges .<br><br>to recap , the estimator runs the combinations below from top to bottom in a step-by-step manner until a plausible hostname is found .<br><br>for future reference , we give names to these steps , where le and ue refer to local and union estimators , and nte refers to no ttl expiration .<br><br>for instance , the estimator le-nte ( local estimator with no ttl expiration ) starts with the first step and continues to the second step until at least one candidate hostname is found , but will not proceed to the third and fourth steps .<br><br>we will examine the accuracies of these estimators to study the factors that contribute to improve the estimation accuracies .<br><br>stepmleconstraintname1steq ( 1 ) eq ( 2 ) le2ndeq ( 1 ) eq ( 3 ) le-nte3rdeq ( 5 ) eq ( 4 ) ue4theq ( 5 ) eq ( 6 ) ue-nte finally , we note the time complexity of the union estimators .<br><br>in the union dng , a single-source path search from s with reverse edges requires o ( |e| ) on a directed acyclic graph with topological sort , and frequency lookups are executed for n nv .<br><br>therefore , the time complexity of union estimators is o ( |v|+|e| ) .<br><br>however , we empirically revealed that the actual mean time complexity is much smaller than this worst-case upper bound , and is close to o ( |vc|+|ec| ) because majority of hostnames can be estimated with le and le-nte as we shall show in section4 .<br><br>the details are omitted due to the space limitation .<br><br>to calculate the likelihood probabilities , we make use of empirical data .<br><br>let fc ( n , s ) denote the frequency of dns messages queried by client c for hostname n with resolved address s. using fc ( n , s ) , eq1 can be calculated as argmaxnnpr ( n , c , s ) =argmaxnnfc ( n , s ) .similarly , eq5 can be calculated as argmaxnnpr ( n , s ) =argmaxnnf ( n , s ) , where f ( n , s ) =ccfc ( n , s ) .<br><br>the method to update the frequency will be shown in the next subsection .<br><br>the updater updates dng gc and frequency fc when it receives a dns response .<br><br>a dns response is associated with client c and queried hostname n. the response also includes a set of a records and another set of cname records .<br><br>let these sets be a and m , respectively .<br><br>an a record associates hostname n and server address s , while a cname record associates two hostnames n and n. let these records be ( n , s ) a and ( n , n ) m , respectively .<br><br>due to the existence of short ttl value set for an a record , a client often resolves an intermediate hostname ( ie , cname ) instead of the original one .<br><br>in such a case , the frequency of an original hostname is undervalued .<br><br>to cope with such a case , sfmap increments the frequencies of all original hostnames that can reach to the queried hostname .<br><br>let a set of edges be ec= { ( n , n ) , ( n , n ) , ( n , s ) } , where n is a cname of n or n. if n=n is queried , the updater increments fc ( n , s ) and fc ( n , s ) , instead of fc ( n , s ) .<br><br>note that we assume that original hostnames should be leaf vertices on a dng ( a leaf is a vertex without incoming edge ) .<br><br>in fact , more than 99.7 % of requested hostnames are leaf vertices in our observations .<br><br>algorithm1 presents an algorithm that updates gc and fc upon receiving a dns response , ( c , n , a , m ) .<br><br>we discount the incremental value by the number of ( n , s ) pairs at line 7 , because the algorithm increments fc for all n v reachable to n and for all s in a .<br><br>at line 3 , we update the expiration time of edge ( u , v ) .<br><br>in addition to algorithm1 , the updater periodically checks the ttl expiration for all edges .<br><br>if the dns ttl expires for an edge ( u , v ) , the edge will be removed .<br><br>the time complexity of maintenance is o ( |vc| ) for the loop at line 5 , assuming o ( |a| ) =o ( |m| ) =o ( 1 ) .<br><br></p><br><br><h3>PERFORMANCE EVALUATION</h3><p>in this section , we evaluate the performance of the sfmap framework through extensive experiments .<br><br>on the basis of the experiment results , we also aim to identify the optimum setting for the sfmap framework .<br><br>to these ends , we first describe the data sets used and present some basic statistics derived from the data .<br><br>next , we evaluate the estimation accuracy of sfmap in various conditions .<br><br>for reference , we compare the performance of sfmap with dn-hunter [ 2 ] .<br><br>finally , we summarize the lessons learned for optimizing the sfmap framework .<br><br>to investigate the effectiveness of sfmap , we used the two datasets , lab and prod , which are the packet traces collected from a gateway router of local area network used by a research group and a gateway router of middle-scale production network , respectively .<br><br>the basic statistics of the datasets are summarized in table1 .<br><br>as is shown in table1 , the datasets cover two different scales , small and middle .<br><br>both datasets have same time length , 12h .<br><br>of the 12h , the last two hours are used to examine the accuracy ; ie , the first 10h are used for warm-up phase .<br><br>we adopted the length of warm-up from the observation of ttl distribution shown in fig2 ; ie , majority of the dns resource records had ttl values less than 10h .<br><br>here , we present the characteristics of dngs derived from our datasets .<br><br>table2 presents the statistics of the dngs .<br><br>for brevity , we omit dngs with ttl expiration because these dngs should be smaller than those without ttl expiration .<br><br>as is shown in the table , union dngs have fewer nodes and edges .<br><br>for instance , since the number of clients for the lab dataset is 10 ( see table1 ) , the total number of nodes in the local dngs should be 10460=4600 .<br><br>thus , the number of total nodes in the union dng ( =2849 ) is less than the number of total nodes in the local dngs .<br><br>this observation implies that ( 1 ) each client-server pair in the local dngs has duplicate nodes and edges , and ( 2 ) the union dngs can be maintained with less memory .<br><br>fig5 shows the cdf of the number of candidate hostnames/domains for each http request .<br><br>here , a candidate domain is the domain of a host ( public suffix ) .<br><br>the results suggest that roughly 15 % of the http requests have multiple candidates ; ie , we must statistically estimate the original hostname from these candidates .<br><br>our methodology was evaluated using the two datasets .<br><br>we make use of http as a means to evaluate the accuracy of our methodologies .<br><br>the ground truth was obtained from http request headers , which contain hostname information .<br><br>we note that although the distributions of hostnames could be different between http and https , the fundamental mechanism of resolving hostname before starting http/https communication should be identical .<br><br>from the packet traces , we read dns packets to build and update the dngs .<br><br>for each http request pair ( c , s ) , we estimate the hostname and compare it against the ground truth .<br><br>for comparison purposes , we implemented dn-hunter [ 2 ] .<br><br>dn-hunter has a single parameter that determines the size of memory , which keeps track of tuples of ( c , s , n ) , where n is a hostname .<br><br>to obtain the highest performance of dn-hunter , we set infinite amount of memory size .<br><br>we note that this configuration did not overflow physical memory we used in our experiments .<br><br>tables3 and4 summarize the results , where we use the notations introduced in section3.3 .<br><br>table3 shows the estimation accuracies in the context of exact matching , and table4 relaxes matching using a public suffix [ 12 ] ; ie , we can see that aaa.example.com and bbb.example.com are matched in the context of the public suffix .<br><br>using the public suffix matching allows us to distinguish hostnames with different domains , eg , youtube.com and google.com .<br><br>first , the accuracies were improved for estimators with no ttl expiration ( nte ) .<br><br>this observation suggests that there are a non-negligible number of dns implementations that ignore ttl settings , which agrees with a previous report [ 4 ] .<br><br>second , the union dng also contributed to improve the accuracy .<br><br>this observation suggests that using other clients information is useful in improving the accuracy when no other hint is available .<br><br>third , if we allow public suffix matching , accuracies are further improved for all the estimators .<br><br>the ue-nte achieved roughly 95 % of accuracy for both datasets .<br><br>finally , the ue-nte outperformed dn-hunter .<br><br>for the exact matching experiments , while the estimation error rates of dn-hunter were 15-32 % , the estimation error rates of ue-nte were 89 % .<br><br>thus , ue-nte successfully reduced the error rates by 5070 % .<br><br>dn-hunter returns a single hostname given a client-server pair ; however , if there are multiple candidate hostnames , sfmap can return several hostnames with the highest likelihood probabilities .<br><br>table5 shows the results where we accept the top three hostnames as estimation .<br><br>notably , accuracies exceed 9698 % for exact matching if we pick up the top three hostnames .<br><br>we note that in most cases , the hostnames ranked in the top three look similar .<br><br>for instance , the top three hostnames are : pagead2.googlesyndication.com , pubads.g.doubleclick.net , and googleads.g.doubleclick.net , which are all attributed to ad network services .<br><br>thus , by extending the candidate hostnames , we can establish better estimations that work in practice .<br><br>this extension is acceptable for our original motivation ; ie , understanding the mix of https traffic .<br><br>we summarize the lessons learned for optimizing the sfmap framework as follows .<br><br>first , due to the existence of many dns implementations that likely ignore dns ttl , we found that omitting ttl expiration on dng nodes leads to the improvement of the estimation accuracy .<br><br>second , we found that introducing the union dng improves the estimation accuracy .<br><br>the observation suggests that the graph structure plays a key role in extracting other clients information , which enables us to complement the incomplete dns information .<br><br>thus , we conclude that ue-nte is the optimal setting for the sfmap framework .<br><br>as the internet traffic continuously keeps growing , we may want to ask whether the sfmap framework works for large-scale networks .<br><br>to answer the question , we empirically study the scalability of the sfmap framework .<br><br>in this section , we first study the memory consumption and data processing time of the sfmap framework , using the three data sets , lab , prod , and access .<br><br>the last one is a new , and large data set we describe later .<br><br>we then introduce two techniques , path sampling and graph partitioning , which improve the scalability of the sfmap framework .<br><br>we validate the scalability of the sfmap framework , using the access dataset .<br><br>throughout this section , we adopt the ue-nte , which we validated as the optimum setting in the previous section .<br><br>the access data is a set of packet traces collected at a large-scale internet access link .<br><br>the vantage point was located at a gateway point where several thousands of users reside .<br><br>due to the nda with the data provider , other details such as exact number of users , type of the network link , network topology , and the traffic statistics such as top popular domain names can not be disclosed in this paper .<br><br>tables6 and 7 summarize the basic statistics of the access dataset .<br><br>to make the condition become consistent among the three data sets , the lengths of learning time and estimating time were configured to 12h and 2h , respectively .<br><br>compared with the statistics presented in table1 , we notice that the scale of the access data is roughly 100 times larger than that of prod data set ; ie , the data set is useful in examining the scalability of the sfmap framework .<br><br>using the three datasets , lab , prod , and access , we study the memory consumption and data processing time of the sfmap framework .<br><br>for this experiment , we used our implementation of the sfmap framework written in python .<br><br>we note that the implementation has much room for improvement in terms of optimizing resource management .<br><br>table8 shows the amount of time/memory to process the entire data , including data for warm-up .<br><br>the results demonstrate that our implementation of sfmap works within a reasonable amount of memory for lab and prod data sets , ie , less than 40 mb for lab and less than 700 mb for prod .<br><br>for the access data set , 28 gb of memory space was needed for our implementation .<br><br>although that amount of memory consumption is affordable for modern commodity servers , we need to make the memory management scheme scalable to the size of data .<br><br>also , for lab and prod , the processing time is much shorter than the actual measurement length , 12h .<br><br>thus , sfmap should work in a real-time fashion .<br><br>however , for the access data set , the data processing time was longer than 12h ; ie , it will not work in a real-time fashion .<br><br>thus , our experiment suggests that we need to make the sfmap framework scalable both in memory consumption and data processing time .<br><br>given the results shown above , we inspected the root cause that made the data processing time much longer for the access data set .<br><br>it turned out that the cost of the sfmap framework is associated with the size of dngs ; ie , as dngs grow over time , some nodes in the graph could have a large number of edges .<br><br>since the candidate hostname search algorithm checks all the paths on the graph for a given pair of client and server , the amount of processing time increases as the number of such high degree nodes increases .<br><br>to solve the problem of the high degree nodes , we adopt a simple approach sampling .<br><br>as we explained in section3.3 , for a given client-server pair ( c , s ) , the sfmap algorithm enumerates a set of candidate hostnames , n by traversing a dng .<br><br>our approach is to limit the cost of traversing a dng with sampling .<br><br>when traversing a dng , if a number of paths for a given node exceeds a threshold , h , we stop the traversal and select candidates only on the traversed paths .<br><br>we empirically determined the threshold as h=16 .<br><br>with this threshold , h=16 , a probability that a path is not traversed is smaller than 0.01 .<br><br>we have validated that choosing other thresholds such as h=8 or h=32 did not affect the results .<br><br>the path sampling approach works as follows : on the dng traversal , we inverse edge directions and walk on dngs from ( c , s ) to leaves in the depth first manner .<br><br>at each node , we randomly choose a next hop from the neighbor nodes that are kept in a hashtable .<br><br>after traversing at most h paths , we stop the traversal and extract candidate hostnames from the traversed paths .<br><br>in addition to the path sampling , we introduce graph partitioning , which is an approach to reduce the cost of traversing a large union dng .<br><br>that is , instead of keeping a single , large union dng , we create several small union dngs , each of which are associated with a subset of randomly chosen clients .<br><br>the drawback of this approach is that by partitioning a union dng into small graphs , we may lose some information ; ie , some paths that were in the original dng will be missed .<br><br>we will study how the graph partitioning affect the estimation accuracy .<br><br>the advantage of the graph partitioning is that the tasks of processing sub union dngs can be parallelized and/or distributed .<br><br>as recent high-speed network monitoring frameworks such as [ 6 ] and [ 8 ] have revealed , use of the parallel/distributed computing approach is one of the promising ways to achieve high scalability .<br><br>fig6 shows the results .<br><br>the threshold for sampling paths was set to h=16 .<br><br>the number of partitioned dngs was set to m=1,2,4,8,16 .<br><br>as the number of partitioned dngs increases , both the mean data processing time per dng and mean memory consumption per dng decrease .<br><br>the regression analysis shows that tm1.7 and sm1.0 , where t and s are time and memory size , respectively .<br><br>the fact that both time and memory obey the power-law with the exponent smaller than 1.0 indicates the good scalability of the graph partitioning approach .<br><br>for instance , if we adopt m=16 , which is the affordable number of processes that can run on a modern multi-core processor , the mean time required to process 12h of traffic data is less than 10 mins .<br><br>similarly , if we adopt m=16 , the mean memory consumption required to keep a partitioned dng fits into a 2 gb of memory space .<br><br>these observations clearly showed that the partitioning method enables us to make the sfmap framework scalable both in data processing time and memory consumption .<br><br>we further demonstrate that data processing time of the sfmap framework is actually short .<br><br>to this end , we measured time needed for processing a packet ; ie , loading a dns query and searching for a http request .<br><br>fig7 shows the log-log complementary cumulative distribution functions ( ccdfs ) of the measured time for the access data set .<br><br>here , number of partition was set to m=16 .<br><br>as shown , more than 99/99.9 % of data processing completed within 10/100ms .<br><br>in fact , the mean processing times were 0.21ms for loading dns information and 0.53ms for looking up names for http requests , respectively .<br><br>although there are a very few number of cases that took longer processing time ( 3s ) , we can safely skip such cases by setting appropriate timeout ( say , 1s ) without affecting the overall accuracy since such cases occurred infrequently ( p < 104 ) .<br><br>we also note that the sfmap framework is applied to incoming http headers but not to all the tcp packet headers .<br><br>thus , we conclude that our techniques enable the sfmap framework to monitor traffic in a real-time fashion .<br><br>finally , we study how the partitioning method affects the estimation accuracy .<br><br>table9 shows the results where we accept the top three hostnames as estimation like we did in table5 .<br><br>although the estimation accuracy degrades as m increases , the difference is small ( 1 % ) .<br><br>if we adopt m=16 , the accuracy exceeds 92.9 % for public suffix matching if we pick up the top three hostnames .<br><br>as we mentioned earlier , in most cases , the hostnames ranked in the top three look similar .<br><br>we also note that the obtained results outperformed the dn-hunter , which established the estimation accuracy of 77.7 % for the same data .<br><br>these results demonstrate that while the partitioning method can establish good scalability in both time and space , it can also achieve the high estimation accuracy .<br><br></p><br><br><h3>DISCUSSION</h3><p>here , we discuss the limitations of the proposed sfmap framework .<br><br>we also outline several future research directions that can help extend our framework .<br><br>by carefully examining the estimation results , we found several intrinsic sources of misclassification .<br><br>there are several factors that are associated with the incomplete measurements .<br><br>as we mentioned before , the first factor is the existence of aggressive dns caching mechanisms that ignore dns ttl setting .<br><br>the second factor we found through this study was mobility of terminals ; ie , an ip address had already been resolved in other network before the terminal arrived to the vantage point .<br><br>the third factor we found was the use of an ip address in the uri .<br><br>we found a non-negligible number of http requests had such uris .<br><br>we manually inspected the cases and found that there are several applications that likely hard-coded an ip address ; thus , they never send dns queries .<br><br>although these are not the controlling factors today , we may need to address them if such deployments become popular in future .<br><br>as shown in section5 , the partitioning method enables us to apply the distributed computing approach to cope with more large-scale traffic data .<br><br>however , if the total size of dngs becomes large enough to press the capacity of the entire memory in a distributed monitoring system , we will need to eliminate old records .<br><br>that is , instead of keeping all of the records for a certain amount of time , eg , 12h , we might want to quickly delete old records that are less-likely to be reused in the future .<br><br>for instance , we can safely remove entries such as those with private/bogus ip addresses .<br><br>also , once we learned that a pair of hostname and server ip address is fixed and stable , we can keep that rule without using dng .<br><br>finally , if memory space is fully utilized despite the above approaches , we will eliminate hostnames/ip addresses that were registered but have not been reused again .<br><br>the actual evaluation of memory management schemes is left for future studies .<br><br>we note that in the traffic flow monitoring systems such as netflow , similar memory management approaches have already been used .<br><br>another possible solution would be to build a new algorithm that can maintain and update dngs in a more compact data structure .<br><br>the topic is also left for future studies .<br><br>finally , adopting packet/flow sampling techniques is also a possible solution .<br><br>as previous studies such as [ 5 ] and [ 11 ] have revealed , sample-based traffic monitoring is a promising approach because it can drastically reduce the amount of packets or flows to be processed while necessary traffic statistics can be reverted from the sampled packets or flows .<br><br></p><br><br><h3>SUMMARY</h3><p>the sfmap hostname estimation framework was presented .<br><br>sfmap enables network operators to estimate the hostnames of https traffic by observing dns queries/responses .<br><br>to tackle the challenges that arise from the recent dynamic deployment and diverse implementations of dns ecosystems , the proposed sfmap framework runs on top of a single key component ; ie , a dng , which is a formal expression that characterizes the highly dynamic and diverse nature of dns mechanisms .<br><br>from extensive analyses using real packet traces collected from two distinct locations with different network scales , we have demonstrated that sfmap has good estimation accuracy and can outperform dn-hunter , which is a state-of-the-art estimation technique .<br><br>we also introduced two techniques , path sampling and graph partitioning , which make the sfmap framework scalable in both data processing time and size of memory without losing the high accuracy .<br><br><span style="background-color:hsla(11.4285714286, 100%, 50%, 0.5)">&nbspwe validated the effectiveness of the approach using large-scale traffic data collected at a gateway point of internet access links . </span><br><br></p><br><br><h3>ACKNOWLEDGMENTS</h3><p>we thank the workshop chairs of the 7th international workshop on traffic monitoring and analysis ( tma 2015 ) for providing us with the opportunity to publish the extended version of our work .<br><br>a part of this work was supported by jsps kakenhi grant number 25880020 .<br><br></p><br><br><h3>REFERENCES</h3></div></body></html>